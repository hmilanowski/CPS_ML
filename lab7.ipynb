{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_cls = 20\n",
    "# Number of samples in each class.\n",
    "np.random.seed(1) # for reproducibility\n",
    "x11 = np.random.normal(0.5, 1, (n_cls, 1))\n",
    "np.random.seed(2) # for reproducibility\n",
    "x12 = np.random.normal(0.4, 1, (n_cls, 1))\n",
    "np.random.seed(3) # for reproducibility\n",
    "x21 = np.random.normal(-0.3, 1, (n_cls, 1))\n",
    "np.random.seed(4) # for reproducibility\n",
    "x22 = np.random.normal(-0.5, 1, (n_cls, 1))\n",
    "X = np.vstack((np.hstack((x11, x12)), np.hstack((x21, x22))))\n",
    "y = np.hstack((-1 * np.ones(n_cls), +1 * np.ones(n_cls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train SVM classifierfrom sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C=1000, random_state=1, probability=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, kernel='linear', probability=True, random_state=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C=1000, random_state=1, probability=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result on the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'x2')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(1)\n",
    "plt.clf()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "linestyles=['--', '-', '--'])\n",
    "# plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify new examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_new = np.array([\n",
    "[1, -0.4],\n",
    "[-1, -0.85],\n",
    "])\n",
    "y_new = clf.predict(X_new)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], c=y_new, s=100)\n",
    "fig.canvas.draw()\n",
    "fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from sample email (emailSample1.txt)\n",
      "\n",
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "anyon \n",
      "know \n",
      "how \n",
      "much it \n",
      "cost to \n",
      "host a \n",
      "web \n",
      "portal \n",
      "well it \n",
      "depend on \n",
      "how \n",
      "mani \n",
      "visitor \n",
      "your \n",
      "expect \n",
      "thi \n",
      "can be \n",
      "anywher \n",
      "from \n",
      "less \n",
      "than \n",
      "number \n",
      "buck a \n",
      "month to \n",
      "a \n",
      "coupl of \n",
      "dollarnumb \n",
      "you \n",
      "should \n",
      "checkout \n",
      "httpaddr if \n",
      "your \n",
      "run \n",
      "someth \n",
      "big to \n",
      "unsubscrib \n",
      "yourself \n",
      "from \n",
      "thi \n",
      "mail \n",
      "list \n",
      "send an \n",
      "email to \n",
      "emailaddr \n",
      "\n",
      "=========================\n",
      "\n",
      "(1899, 1)\n",
      "Length of feature vector: 1\n",
      "\n",
      "Number of non-zero entries: 42\n",
      "\n",
      "\n",
      "Loading the training dataset...\n",
      "The training dataset was loaded.\n",
      "xtr shape:  (4000, 1899)\n",
      "\n",
      "Training Linear SVM (Spam Classification)\n",
      "\n",
      "(this may take 1 to 2 minutes) ...\n",
      "\n",
      "Training Accuracy: 99.83%\n",
      "\n",
      "\n",
      "Evaluating the trained Linear SVM on a test set ...\n",
      "\n",
      "Test Accuracy: 98.90%\n",
      "\n",
      "\n",
      "Top predictors of spam: \n",
      "\n",
      "Word: otherwis | Weight: [0.50061374]\n",
      "Word: clearli | Weight: [0.46591639]\n",
      "Word: remot | Weight: [0.42286912]\n",
      "Word: gt | Weight: [0.3836216]\n",
      "Word: visa | Weight: [0.3677104]\n",
      "Word: base | Weight: [0.3450641]\n",
      "Word: doesn | Weight: [0.32363204]\n",
      "Word: wife | Weight: [0.26972411]\n",
      "Word: previous | Weight: [0.26729771]\n",
      "Word: player | Weight: [0.26116889]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "best buy \n",
      "viagra gener \n",
      "onlin viagra \n",
      "numbermg x \n",
      "number pill \n",
      "dollarnumb free \n",
      "pill reorder \n",
      "discount top \n",
      "sell number \n",
      "qualiti satisfact \n",
      "guarante we \n",
      "accept visa \n",
      "master echeck \n",
      "payment number \n",
      "satisfi custom \n",
      "httpaddr \n",
      "\n",
      "=========================\n",
      "\n",
      "\n",
      "Processed data/spamSample2.txt\n",
      "\n",
      "Spam Classification: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run -i svm_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\n",
      "# -*- coding: utf-8 -*-\n",
      "import csv\n",
      "\n",
      "from typing import Dict\n",
      "\n",
      "\n",
      "def get_vocabulary_dict() -> Dict[int, str]:\n",
      "    \"\"\"Read the fixed vocabulary list from the datafile and return.\n",
      "\n",
      "    :return: a dictionary of words mapped to their indexes\n",
      "    \"\"\"\n",
      "\n",
      "    # FIXME: Parse data from the 'data/vocab.txt' file.\n",
      "    # - The file is saved in tab-separated values (TSV) format.\n",
      "    # - Each line contains a word's ID and the word itself.\n",
      "    # The output dictionary should map word's ID on the word itself, e.g.:\n",
      "    #   {1: 'aa', 2: 'ab', ...}\n",
      "    \n",
      "    num_word = dict()\n",
      "    \n",
      "    with open('data/vocab.txt', newline='') as csv_file:\n",
      "        spamreader = csv.reader(csv_file, delimiter=' ', quotechar='|')\n",
      "        for row in spamreader:\n",
      "            split_str = row[0].split()\n",
      "            number = int(split_str[0])\n",
      "            word = split_str[1]\n",
      "            num_word[number] = word\n",
      "\n",
      "    return num_word\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('get_vocabulary_dict.py') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\n",
      "# -*- coding: utf-8 -*-\n",
      "import re\n",
      "from typing import List\n",
      "\n",
      "from nltk import PorterStemmer\n",
      "\n",
      "from get_vocabulary_dict import get_vocabulary_dict\n",
      "\n",
      "\n",
      "def process_email(email_contents: str) -> List[int]:\n",
      "    \"\"\"Pre-process the body of an email and return a list of indices of the\n",
      "    words contained in the email.\n",
      "\n",
      "    :param email_contents: the body of an email\n",
      "    :return: a list of indices of the words contained in the email\n",
      "    \"\"\"\n",
      "\n",
      "    # FIXME: Load the vocabulary.\n",
      "    vocabulary_dict = get_vocabulary_dict()\n",
      "\n",
      "    # FIXME: Initialize the return value.\n",
      "    word_indices = []\n",
      "\n",
      "    # ========================== Preprocess Email ===========================\n",
      "\n",
      "    # Find the Headers ( \\n\\n and remove )\n",
      "    # Uncomment the following lines if you are working with raw emails with the\n",
      "    # full headers\n",
      "\n",
      "    # header_token = '\\n\\n'\n",
      "    # header_start = email_contents.find(header_token)\n",
      "    # email_contents = email_contents[header_start+len(header_token):]\n",
      "\n",
      "    # FIXME: Convert email content to lower case.\n",
      "    email_contents = email_contents.lower()\n",
      "\n",
      "    # Strip all HTML\n",
      "    # Looks for any expression that starts with < and ends with > and replace\n",
      "    # and does not have any < or > in the tag it with a space\n",
      "    email_contents = re.sub('<[^<>]+>', ' ', email_contents)\n",
      "\n",
      "    # FIXME: Handle numbers.\n",
      "    # Convert all sequences of digits (0-9) to a 'number' token.\n",
      "    email_contents = re.sub('[0-9]+', 'number', email_contents)\n",
      "\n",
      "    # FIXME: Handle URLs.\n",
      "    # Convert all strings starting with http:// or https:// to a 'httpaddr' token.\n",
      "    email_contents = re.sub('(http://|https://).+', 'httpaddr', email_contents)\n",
      "\n",
      "    # FIXME: Handle email addresses.\n",
      "    # Convert all strings with @ in the middle to a 'emailaddr' token.\n",
      "    email_contents = re.sub('.+@.+', 'emailaddr', email_contents)\n",
      "\n",
      "    # FIXME: Handle $ sign\n",
      "    # Convert all sequences of $ signs to a 'dollar' token.\n",
      "    email_contents = re.sub('[$]', 'dollar', email_contents)\n",
      "\n",
      "    # ========================== Tokenize Email ===========================\n",
      "\n",
      "    # Output the email to screen as well\n",
      "    print('\\n==== Processed Email ====\\n\\n')\n",
      "\n",
      "    # Process file\n",
      "    col = 0\n",
      "\n",
      "    # Tokenize and also get rid of any punctuation\n",
      "    tokens = re.split('[ @$/#.-:&*\\+=\\[\\]?!\\(\\)\\{\\},''\">_<;#\\n\\r]', email_contents)\n",
      "\n",
      "    for token in tokens:\n",
      "\n",
      "        # Remove any non alphanumeric characters\n",
      "        token = re.sub('[^a-zA-Z0-9]', '', token)\n",
      "\n",
      "        # Stem the word \n",
      "        token = PorterStemmer().stem(token.strip())\n",
      "\n",
      "        # Skip the word if it is too short\n",
      "        if len(token) < 1:\n",
      "            continue\n",
      "\n",
      "        # Look up the word in the dictionary and add to word_indices if\n",
      "        # found\n",
      "        # FIXME: ======================= YOUR CODE HERE ======================\n",
      "        \n",
      "        for index in vocabulary_dict:\n",
      "            if vocabulary_dict[index] == token:\n",
      "                word_indices.append(index)\n",
      "        \n",
      "        # Instructions: Fill in this function to add the index of str to\n",
      "        #               word_indices if it is in the vocabulary. At this point\n",
      "        #               of the code, you have a stemmed word from the email in\n",
      "        #               the variable str. You should look up str in the\n",
      "        #               vocabulary list (vocabulary_dict). If a match exists, you\n",
      "        #               should add the index of the word to the word_indices\n",
      "        #               vector. Concretely, if str = 'action', then you should\n",
      "        #               look up the vocabulary list to find where in vocabulary_dict\n",
      "        #               'action' appears. For example, if vocabulary_dict{18} =\n",
      "        #               'action', then, you should add 18 to the word_indices \n",
      "        #               vector (e.g., word_indices = [word_indices ; 18]; ).\n",
      "        # \n",
      "        # Note: vocabulary_dict{idx} returns a the word with index idx in the\n",
      "        #       vocabulary list.\n",
      "        # \n",
      "        # Note: You can use strcmp(str1, str2) to compare two strings (str1 and\n",
      "        #       str2). It will return 1 only if the two strings are equivalent.\n",
      "        #\n",
      "\n",
      "        # ========================= END OF YOUR CODE ==========================\n",
      "\n",
      "        # Print to screen, ensuring that the output lines are not too long\n",
      "        if (col + len(token) + 1) > 78:\n",
      "            print('')\n",
      "            col = 0\n",
      "        print('{} '.format(token), end='', flush=True)\n",
      "        col = col + len(tokens) + 1\n",
      "\n",
      "    # Print footer\n",
      "    print('\\n\\n=========================\\n')\n",
      "\n",
      "    return word_indices\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('process_email.py') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\n",
      "# -*- coding: utf-8 -*-\n",
      "from typing import List\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def email_features(word_indices: List[int]) -> np.ndarray:\n",
      "    \"\"\"Convert a list of word IDs into a feature vector.\n",
      "\n",
      "    :param word_indices: a list of word IDs\n",
      "    :return: a feature vector from the word indices (a row vector)\n",
      "    \"\"\"\n",
      "\n",
      "    # Total number of words in the dictionary\n",
      "    n_words = 1899\n",
      "\n",
      "    # FIXME: ====================== YOUR CODE HERE ============================\n",
      "    # Instructions: Fill in this function to return a feature vector for the\n",
      "    #               given email (word_indices). To help make it easier to \n",
      "    #               process the emails, we have have already pre-processed each\n",
      "    #               email and converted each word in the email into an index in\n",
      "    #               a fixed dictionary (of 1899 words). The variable\n",
      "    #               word_indices contains the list of indices of the words\n",
      "    #               which occur in one email.\n",
      "    # \n",
      "    #               Concretely, if an email has the text:\n",
      "    #\n",
      "    #                  The quick brown fox jumped over the lazy dog.\n",
      "    #\n",
      "    #               Then, the word_indices vector for this text might look \n",
      "    #               like:\n",
      "    #               \n",
      "    #                   60  100   33   44   10     53  60  58   5\n",
      "    #\n",
      "    #               where, we have mapped each word onto a number, for example:\n",
      "    #\n",
      "    #                   the   -- 60\n",
      "    #                   quick -- 100\n",
      "    #                   ...\n",
      "    #\n",
      "    #              (note: the above numbers are just an example and are not the\n",
      "    #               actual mappings).\n",
      "    #\n",
      "    #              Your task is take one such word_indices vector and construct\n",
      "    #              a binary feature vector that indicates whether a particular\n",
      "    #              word occurs in the email. That is, x(i) = 1 when word i\n",
      "    #              is present in the email. Concretely, if the word 'the' (say,\n",
      "    #              index 60) appears in the email, then x(60) = 1. The feature\n",
      "    #              vector should look like:\n",
      "    #\n",
      "    #              x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..];\n",
      "    #\n",
      "    # =========================================================================\n",
      "\n",
      "    feature_vector = np.zeros((n_words, 1))\n",
      "    \n",
      "    for number in word_indices:\n",
      "        feature_vector[number] = 1\n",
      "        \n",
      "    return feature_vector\n",
      "    # =========================== END OF YOUR CODE ============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('email_features.py') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\n",
      "# -*- coding: utf-8 -*-\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "\n",
      "from process_email import process_email\n",
      "from email_features import email_features\n",
      "from get_vocabulary_dict import get_vocabulary_dict\n",
      "\n",
      "\n",
      "def read_file(file_path: str) -> str:\n",
      "    \"\"\"Return the content of the text file under the given path.\n",
      "\n",
      "    :param file_path: path to the file\n",
      "    :return: file content\n",
      "    \"\"\"\n",
      "\n",
      "    with open(file_path) as f:\n",
      "        file_content = f.read()\n",
      "    return file_content\n",
      "\n",
      "\n",
      "# %% ==================== Part 1: Email Preprocessing ====================\n",
      "#  To use an SVM to classify emails into Spam v.s. Non-Spam, you first need\n",
      "#  to convert each email into a vector of features. In this part, you will\n",
      "#  implement the preprocessing steps for each email. You should\n",
      "#  complete the code in process_email.py to produce a word indices vector\n",
      "#  for a given email.\n",
      "\n",
      "# print('\\nPreprocessing sample email (emailSample1.txt)\\n')\n",
      "\n",
      "# Print Stats\n",
      "# print('Word Indices: \\n')\n",
      "# print(word_indices)\n",
      "# print('\\n\\n')\n",
      "\n",
      "# input('Program paused. Press enter to continue.\\n')\n",
      "\n",
      "# %% ==================== Part 2: Feature Extraction ====================\n",
      "#  Now, you will convert each email into a vector of features in R^n. \n",
      "#  You should complete the code in email_features.py to produce a feature\n",
      "#  vector for a given email.\n",
      "\n",
      "print('\\nExtracting features from sample email (emailSample1.txt)\\n')\n",
      "\n",
      "# Extract Features\n",
      "file_contents = read_file('data/emailSample1.txt')\n",
      "word_indices = process_email(file_contents)\n",
      "features = email_features(word_indices)\n",
      "print(features.shape)\n",
      "# Print Stats\n",
      "print('Length of feature vector: {}\\n'.format(len(features[0])))\n",
      "print('Number of non-zero entries: {}\\n'.format(sum(sum(features > 0))))\n",
      "\n",
      "# input('Program paused. Press enter to continue.\\n')\n",
      "\n",
      "# %% =========== Part 3: Train Linear SVM for Spam Classification ========\n",
      "#  In this section, you will train a linear classifier to determine if an\n",
      "#  email is Spam or Not-Spam.\n",
      "\n",
      "# Load the Spam Email dataset\n",
      "# You will have X, y in your environment\n",
      "\n",
      "print('\\nLoading the training dataset...')\n",
      "X_train = np.genfromtxt('data/spamTrain_X.csv', delimiter=',')\n",
      "y_train = np.genfromtxt('data/spamTrain_y.csv', delimiter=',')\n",
      "print('The training dataset was loaded.')\n",
      "print('xtr shape: ', X_train.shape)\n",
      "print('\\nTraining Linear SVM (Spam Classification)\\n')\n",
      "print('(this may take 1 to 2 minutes) ...\\n')\n",
      "\n",
      "# Create a linear SVC classifier (with C = 0.1).\n",
      "clf = svm.SVC(C=0.1, kernel='linear')\n",
      "\n",
      "# Fit the SVC model using the training data.\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict the labelling.\n",
      "y_pred = clf.predict(X_train)\n",
      "\n",
      "# Compute the training accuracy.\n",
      "\n",
      "acc_train = np.mean(np.equal(y_train, y_pred))\n",
      "print('Training Accuracy: {:.2f}%\\n'.format(acc_train * 100))\n",
      "\n",
      "# %% =================== Part 4: Test Spam Classification ================\n",
      "#  After training the classifier, we can evaluate it on a test set.\n",
      "\n",
      "# Load the test dataset ('data/spamTest_X.csv', 'data/spamTest_y.csv').\n",
      "# You will have Xtest, ytest in your environment\n",
      "X_test = np.genfromtxt('data/spamTest_X.csv', delimiter=',')\n",
      "y_test = np.genfromtxt('data/spamTest_y.csv', delimiter=',')\n",
      "\n",
      "print('\\nEvaluating the trained Linear SVM on a test set ...\\n')\n",
      "\n",
      "# Predict the labelling.\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Compute the training accuracy.\n",
      "acc_test = np.mean(np.equal(y_test, y_pred))\n",
      "print('Test Accuracy: {:.2f}%\\n'.format(acc_test * 100))\n",
      "\n",
      "# input('Program paused. Press enter to continue.\\n')\n",
      "\n",
      "\n",
      "# %% ================= Part 5: Top Predictors of Spam ====================\n",
      "#  Since the model we are training is a linear SVM, we can inspect the\n",
      "#  weights learned by the model to understand better how it is determining\n",
      "#  whether an email is spam or not. The following code finds the words with\n",
      "#  the highest weights in the classifier. Informally, the classifier\n",
      "#  'thinks' that these words are the most likely indicators of spam.\n",
      "\n",
      "# Print the list of 15 most prominent features.\n",
      "# (i.e. words which gives strongest evidence for an email being a spam)\n",
      "# - Obtain the weights of the SVC model.\n",
      "# - Obtain the indices that would sort the weights in the descending order.\n",
      "# - Obtain the vocabulary.\n",
      "\n",
      "weights = clf.coef_\n",
      "idx = np.argsort(-weights)[0]\n",
      "weights = np.transpose(weights)\n",
      "vocabulary_dict = get_vocabulary_dict()\n",
      "print('\\nTop predictors of spam: \\n')\n",
      "for i in range(10):\n",
      "    print('Word: {:} | Weight: {:}'.format(vocabulary_dict[idx[i]], weights[idx[i]]))\n",
      "print('\\n\\n')\n",
      "# input('\\nProgram paused. Press enter to continue.\\n')\n",
      "\n",
      "# %% =================== Part 6: Try Your Own Emails =====================\n",
      "#  Now that you've trained the spam classifier, you can use it on your own\n",
      "#  emails! In the starter code, we have included spamSample1.txt,\n",
      "#  spamSample2.txt, emailSample1.txt and emailSample2.txt as examples. \n",
      "#  The following code reads in one of these emails and then uses your \n",
      "#  learned SVM classifier to determine whether the email is Spam or \n",
      "#  Not Spam\n",
      "\n",
      "# Set the file to be read in (change this to spamSample2.txt,\n",
      "# emailSample1.txt or emailSample2.txt to see different predictions on\n",
      "# different emails types). Try your own emails as well!\n",
      "filename = 'data/spamSample2.txt'\n",
      "\n",
      "# Read and predict\n",
      "file_contents = read_file(filename)\n",
      "word_indices = process_email(file_contents)\n",
      "x = email_features(word_indices)\n",
      "x = np.transpose(x)\n",
      "# Predict the labelling.\n",
      "y_pred = clf.predict(x)\n",
      "\n",
      "print('\\nProcessed {}\\n\\nSpam Classification: {}\\n'.format(filename, y_pred[0] > 0))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('svm_spam.py') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
